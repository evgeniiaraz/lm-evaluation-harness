import datasets
import sacrebleu
from rouge_score import rouge_scorer, scoring
from lm_eval.base import rf, Task
from lm_eval.metrics import mean
import numpy as np

try:
    import bleurt

    HAS_BLEURT = True
except ImportError:
    HAS_BLEURT = False


_CITATION = """
@inproceedings{narayan2018don,
  title={Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1797--1807},
  year={2018}
}
"""


class SummarisationTask(Task):
    def __init__(self):
        super().__init__()
        if not HAS_BLEURT:
            raise ImportError(
                "`TruthfulQAGeneration` requires the `bleurt` package. Please install it with:\n"
                "pip install bleurt@https://github.com/google-research/bleurt/archive/b610120347ef22b494b6d69b4316e303f5932516.zip#egg=bleurt"
                "\nWARNING: Installing any other version of bleurt may result in different results."
            )
        self.bleurt = datasets.load_metric("bleurt")

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        if self.has_training_docs():
            # We cache training documents in `self._training_docs` for faster
            # few-shot processing. If the data is too large to fit in memory,
            # return the training data as a generator instead of a list.
            if self._training_docs is None:
                # TODO: Return the training document generator from `self.dataset`.
                # If you need to process the data, `map` over the documents with
                # the custom processing function, `self._process_doc`. E.g.
                # `map(self._process_doc, self.dataset["validation"])`
                # In most case you can leave this as is unless the dataset split is
                # named differently than the default `"train"`.
                self._training_docs = list(self.dataset["train"])
            return self._training_docs

    def validation_docs(self):
        if self.has_validation_docs():
            # TODO: Return the validation document generator from `self.dataset`.
            # If you need to process the data, `map` over the documents with the
            # custom processing function, `self._process_doc`. E.g.
            # `map(self._process_doc, self.dataset["validation"])`
            # In most case you can leave this as is unless the dataset split is
            # named differently than the default `"validation"`.
            return self.dataset["validation"]

    def test_docs(self):
        if self.has_test_docs():
            # TODO: Return the test document generator from `self.dataset`.
            # If you need to process the data, `map` over the documents with the
            # custom processing function, `self._process_doc`. E.g.
            # `map(self._process_doc, self.dataset["test"])`
            # In most case you can leave this as is unless the dataset split is
            # named differently than the default `"test"`.
            return self.dataset["test"]
            # .select(list(range(500)))

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or
            test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        # until "." because we have single sentence summaries.
        completion = rf.greedy_until(ctx, {"until": ["."]})
        return completion

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        # TODO: For each (sub)metric in the task evaluation, add a key-value pair
        # with the metric name as key and the corresponding metric result as value
        # for the current `doc`.
        completion = [results[0].strip()]
        true_refs = [doc["summary"]]
        # BLEURT scores
        # we only have 1 reference in XSum, so, it's safe to take first score
        bleurt_scores = self.bleurt.compute(
            predictions=completion, references=true_refs
        )["scores"][0]

        # BLEU scores
        bleu_score = self.bleu(true_refs, completion)
        bleu_max = np.nanmax(bleu_score)

        # ROUGE scores
        rouge_scores = self.rouge(true_refs, completion)

        rouge1_score = rouge_scores["rouge1"]
        rouge2_score = rouge_scores["rouge2"]
        rougeL_score = rouge_scores["rougeLsum"]

        return {
            "bleurt": bleurt_scores,
            "bleu": bleu_score,
            "bleu_max": bleu_max,
            "rouge1_score": rouge1_score,
            "rouge2_score": rouge2_score,
            "rougeL_score": rougeL_score,
        }

    def aggregation(self):
        """
        :returns: {str: [metric_score] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metric scores
        """
        # TODO: For each (sub)metric in the task evaluation, add a key-value pair
        # with the metric name as key and an aggregation function as value which
        # determines how to combine results from each document in the dataset.
        # Check `lm_eval.metrics` to find built-in aggregation functions.
        return {
            "bleurt": mean,
            "bleu": mean,
            "bleu_max": mean,
            "rouge1_score": mean,
            "rouge2_score": mean,
            "rougeL_score": mean,
        }

    def higher_is_better(self):
        # TODO: For each (sub)metric in the task evaluation, add a key-value pair
        # with the metric name as key and a `bool` value determining whether or
        # not higher values of that metric are deemed better.
        return {
            "bleurt": True,
            "bleu": True,
            "bleu_max": True,
            "rouge1_score": True,
            "rouge2_score": True,
            "rougeL_score": True,
        }

    def bleu(self, refs, preds):
        """
        From truthful QA evaluation
        Returns `t5` style BLEU scores. See the related implementation:
        https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L41

        :param refs:
            A `list` of `list` of reference `str`s.
        :param preds:
            A `list` of predicted `str`s.
        """
        score = sacrebleu.corpus_bleu(
            preds,
            [refs],
            smooth_method="exp",
            smooth_value=0.0,
            force=False,
            lowercase=False,
            tokenize="intl",
            use_effective_order=False,
        ).score
        return score

    def rouge(self, refs, preds):
        """
        From truthful QA evaluation
        Returns `t5` style ROUGE scores. See the related implementation:
        https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68

        :param refs:
            A `list` of reference `strs`.
        :param preds:
            A `list` of predicted `strs`.
        """
        rouge_types = ["rouge1", "rouge2", "rougeLsum"]
        scorer = rouge_scorer.RougeScorer(rouge_types)
        # Add newlines between sentences to correctly compute `rougeLsum`.

        def _prepare_summary(summary):
            summary = summary.replace(" . ", ".\n")
            return summary

        # Accumulate confidence intervals.
        aggregator = scoring.BootstrapAggregator()
        for ref, pred in zip(refs, preds):
            ref = _prepare_summary(ref)
            pred = _prepare_summary(pred)
            aggregator.add_scores(scorer.score(ref, pred))
        result = aggregator.aggregate()
        return {type: result[type].mid.fmeasure * 100 for type in rouge_types}


class XSum(SummarisationTask):
    """
    XSum: Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization
    https://arxiv.org/pdf/1808.08745.pdf

    XSum (extreme summarisation) dataset is a summarisation dataset which maps BBC articles with their 1 sentence summaries.
    Corpus contains 200K+ articles spanning 12+ domains. The summaries were professionally written by the authors of the paper.

    Homepage: https://github.com/EdinburghNLP/XSum
    """

    VERSION = 0
    DATASET_PATH = "xsum"
    DATASET_NAME = None

    def doc_to_text(self, doc):
        return (
            "Summarize this article in one sentence.\n\n"
            + doc["document"]
            + "\nSummary:\n"
        )

    def doc_to_target(self, doc):
        # TODO: Fill in the `target` ("gold answer") variable.
        # The prepended `" "` is required to space out the `doc_to_text` and
        # `doc_to_target` strings.
        target = doc["summary"]
        return " " + target


class DialogSum(SummarisationTask):
    VERSION = 0
    DATASET_PATH = "knkarthick/dialogsum"
    DATASET_NAME = None

    def test_docs(self):
        test_docs_pre = self.dataset["test"]
        ids_to_filter = [f"test_{i}_3" for i in range(500)]
        test_docs = test_docs_pre.filter(lambda example: example["id"] in ids_to_filter)
        return test_docs

    def doc_to_text(self, doc):
        return "Here is a dialogue:\n" + doc["dialogue"] + "\n\nWrite a short summary!"

    def doc_to_target(self, doc):
        # TODO: Fill in the `target` ("gold answer") variable.
        # The prepended `" "` is required to space out the `doc_to_text` and
        # `doc_to_target` strings.
        target = doc["summary"]
        return " " + target
